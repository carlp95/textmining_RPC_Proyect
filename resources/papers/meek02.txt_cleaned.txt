AbstractWe examine the learning-curve sampling method, an approach for applying machine-learning algorithms to large data sets. The approach is based on the observation thatthe computational cost of learning a model increases as a function of the sample size of thetraining data, whereas the accuracy of a model has diminishing improvements as a functionof sample size. Thus, the learning-curve sampling method monitors the increasing costsand performance as larger and larger amounts of data are used for training, and termi-nates learning when future costs outweigh future benefits. In this paper, we formalize thelearning-curve sampling method and its associated cost-benefit tradeoff in terms of decisiontheory. In addition, we describe the application of the learning-curve sampling method tothe task of model-based clustering via the expectation-maximization (EM) algorithm. Inexperiments on three real data sets, we show that the learning-curve sampling methodproduces models that are nearly as accurate as those trained on complete data sets, butwith dramatically reduced learning times. Finally, we describe an extension of the basiclearning-curve approach for model-based clustering that results in an additional speedup.This extension is based on the observation that the shape of the learning curve for a givenmodel and data set is roughly independent of the number of EM iterations used duringtraining. Thus, we run EM for only a few iterations to decide how many cases to use fortraining, and then run EM to full convergence once the number of cases is selected.Keywords: Learning-curve sampling method, clustering, scalability, decision theory,sampling