AbstractAn approach to the construction of classifiers from imbalanced datasets is described.A dataset is imbalanced if the classification categories are not approximately equally rep-resented. Often real-world data sets are predominately composed of “normal” exampleswith only a small percentage of “abnormal” or “interesting” examples. It is also the casethat the cost of misclassifying an abnormal (interesting) example as a normal example isoften much higher than the cost of the reverse error. Under-sampling of the majority (nor-mal) class has been proposed as a good means of increasing the sensitivity of a classifier tothe minority class. This paper shows that a combination of our method of over-samplingthe minority (abnormal) class and under-sampling the majority (normal) class can achievebetter classifier performance (in ROC space) than only under-sampling the majority class.This paper also shows that a combination of our method of over-sampling the minority classand under-sampling the majority class can achieve better classifier performance (in ROCspace) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our methodof over-sampling the minority class involves creating synthetic minority class examples.Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The methodis evaluated using the area under the Receiver Operating Characteristic curve (AUC) andthe ROC convex hull strategy.