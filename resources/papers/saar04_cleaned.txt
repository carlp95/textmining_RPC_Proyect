Abstract In many cost-sensitive environments class probability estimates are used by deci-sion makers to evaluate the expected utility from a set of alternatives. Supervised learning can be used to build class probability estimates; however, it often is very costly to obtain training data with class labels. Active learning acquires data in-crementally, at each phase identifying especially useful additional data for label-ing, and can be used to economize on examples needed for learning. We outline the critical features of an active learner and present a sampling-based active learn-ing method for estimating class probabilities and class-based rankings. BOOT-STRAP-LV identifies particularly informative new data for learning based on the variance in probability estimates, and uses weighted sampling to account for a potential exampleâ€™s informative value for the rest of the input space. We show empirically that the method reduces the number of data items that must be ob-tained and labeled, across a wide variety of domains. We investigate the contri-bution of the components of the algorithm and show that each provides valuable information to help identify informative examples. We also compare BOOTSTRAP-LV with UNCERTAINTY SAMPLING, an existing active learning method designed to maximize classification accuracy. The results show that BOOTSTRAP-LV uses fewer examples to exhibit a certain estimation accuracy and provide insights to the be-havior of the algorithms. Finally, we experiment with another new active sam-pling algorithm drawing from both UNCERTAINTY SAMPLING and BOOTSTRAP-LV and show that it is significantly more competitive with BOOTSTRAP-LV compared to UNCERTAINTY SAMPLING. The analysis suggests more general implications for im-proving existing active sampling algorithms for classification.  Keywords:   active learning, cost-sensitive learning, class probability estimation, rank-ing, supervised learning, decision trees, uncertainty sampling 