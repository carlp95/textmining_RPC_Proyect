Abstract. This paper shows that the accuracy of learned text classifiers can be improved byaugmenting a small number of labeled training documents with a large pool of unlabeled docu-ments. This is important because in many text classification problems obtaining training labelsis expensive, while large quantities of unlabeled documents are readily available.We introduce an algorithm for learning from labeled and unlabeled documents based on thecombination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm firsttrains a classifier using the available labeled documents, and probabilistically labels the unlabeleddocuments. It then trains a new classifier using the labels for all the documents, and iteratesto convergence. This basic EM procedure works well when the data conform to the generativeassumptions of the model. However these assumptions are often violated in practice, and poorperformance can result. We present two extensions to the algorithm that improve classificationaccuracy under these conditions: (1) a weighting factor to modulate the contribution of theunlabeled data, and (2) the use of multiple mixture components per class. Experimental results,obtained using text from three different real-world tasks, show that the use of unlabeled datareduces classification error by up to 30%.Keywords: text classification, Expectation-Maximization, integrating supervised and unsuper-vised learning, combining labeled and unlabeled data, Bayesian learning