Ecient and Eective Clustering Methods forSpatial Data MiningRaymond T. NgDepartment of Computer ScienceUniversity of British ColumbiaVancouver, B.C., V6T 1Z4,Canada. Jiawei HanySchool of Computing SciencesSimon Fraser UniversityBurnaby, B.C., V5A 1S6,Canada.AbstractSpatial data mining is the discovery of interesting relationships and characteristicsthat may exist implicitly in spatial databases. In this paper, we explore whether clus-tering methods have a role to play in spatial data mining. To this end, we develop anew clustering method called CLARANS which is based on randomized search. Wealso develop two spatial data mining algorithms that use CLARANS. Our analysis andexperiments show that with the assistance of CLARANS, these two algorithms are veryeective and can lead to discoveries that are dicult to nd with current spatial datamining algorithms. Furthermore, experiments conducted to compare the performanceof CLARANS with that of existing clustering methods show that CLARANS is themost ecient.keywords: spatial data mining, clustering algorithms, randomized search1 IntroductionData mining in general is the search for hidden patterns that may exist in large databases.Spatial data mining in particular is the discovery of interesting relationships and character-istics that may exist implicitly in spatial databases. Because of the huge amounts (usually,tera-bytes) of spatial data that may be obtained from satellite images, medical equipments,video cameras, etc., it is costly and often unrealistic for users to examine spatial data indetail. Spatial data mining aims to automate such a knowledge discovery process. Thus, itplays an important role in a) extracting interesting spatial patterns and features; b) captur-ing intrinsic relationships between spatial and non-spatial data; c) presenting data regularityconcisely and at higher conceptual levels; and d) helping to reorganize spatial databases toaccommodate data semantics, as well as to achieve better performance.Research partially sponsored by NSERC Grants OGP0138055 and STR0134419.yResearch partially supported by NSERC Grant OGP03723 and the Centre for Systems Science of SimonFraser University. 1
Many excellent studies on data mining have been conducted, such as those reportedin [1, 2, 4, 7, 11, 13, 15]. [1] considers the problem of inferring classication functions fromsamples; [2] studies the problem of mining association rules between sets of data items; [7]proposes an attribute-oriented approach to knowledge discovery; [11] develops a visual feed-back querying system to support data mining; and [15] includes many interesting studieson various issues in knowledge discovery such as nding functional dependencies betweenattributes. However, most of these studies are concerned with knowledge discovery on non-spatial data, and the study most relevant to our focus here is [13] which studies spatialdata mining. More specically, [13] proposes a spatial data-dominant knowledge-extractionalgorithm and a non-spatial data-dominant one, both of which aim to extract high-level re-lationships between spatial and non-spatial data. However, both algorithms suer from thefollowing problems. First, the user or an expert must provide the algorithms with spatialconcept hierarchies, which may not be available in many applications. Second, both algo-rithms conduct their spatial exploration primarily by merging regions at a certain level ofthe hierarchy to a larger region at a higher level. Thus, the quality of the results producedby both algorithms relies quite crucially on the appropriateness of the hierarchy to the givendata. The problem for most applications is that it is very dicult to know a priori whichhierarchy will be the most appropriate. Discovering this hierarchy may itself be one of thereasons to apply spatial data mining.To deal with these problems, we explore whether cluster analysis techniques are appli-cable. Cluster Analysis is a branch of statistics that in the past three decades has beenintensely studied and successfully applied to many applications. To the spatial data miningtask at hand, the attractiveness of cluster analysis is its ability to nd structures or clustersdirectly from the given data, without relying on any hierarchies. However, cluster analy-sis has been applied rather unsuccessfully in the past to general data mining and machinelearning. The complaints are that cluster analysis algorithms are ineective and inecient.Indeed, for cluster analysis algorithms to work eectively, there need to be a natural notion ofsimilarities among the \objects" to be clustered. And traditional cluster analysis algorithmsare not designed for large data sets, say more than 2000 objects.For spatial data mining, our approach here is to apply cluster analysis only on the spatialattributes, for which natural notions of similarities exist (e.g. Euclidean or Manhattandistances). As will be shown in this paper, in this way, cluster analysis techniques areeective for spatial data mining. As for the eciency concern, we develop our own clusteranalysis algorithm, called CLARANS, which is designed for large data sets. More specically,we will report in this paper: the development of CLARANS, which is based on randomized search and is partlymotivated by two existing algorithms well-known in cluster analysis, called PAM andCLARA; and the development of two spatial mining algorithms SD(CLARANS) and NSD(CLARANS).Given the nature of spatial data mining, and the fact that CLARANS is based on random-ized search, the methodology we have adopted here is one based on experimentation. Inparticular, we will present: 2
 experimental results showing that CLARANS is more ecient than the existing algo-rithms PAM and CLARA; and experimental evidence and analysis demonstrating the eectiveness of SD(CLARANS)and NSD(CLARANS) for spatial data mining.The paper is organized as follows. Section 2 introduces PAM and CLARA. Section3 presents our clustering algorithm CLARANS, as well as experimental results comparingthe performance of CLARANS, PAM and CLARA. Section 4 studies spatial data miningand presents two spatial data mining algorithms, SD(CLARANS) and NSD(CLARANS).Section 5 gives an experimental evaluation on the eectiveness of SD(CLARANS) andNSD(CLARANS) for spatial data mining. Section 6 discusses how SD(CLARANS) andNSD(CLARANS) can assist in further spatial discoveries, and how they can contribute to-wards the building of a general-purpose and powerful spatial data mining package in thefuture.2 Clustering Algorithms based on Partitioning2.1 OverviewIn the past 30 years, cluster analysis has been widely applied to many areas such as medicine(classication of diseases), chemistry (grouping of compounds), social studies (classicationof statistical ndings), and so on. Its main goal is to identify structures or clusters presentin the data. While there is no general denition of a cluster, algorithms have been developedto nd several kinds of clusters: spherical, linear, drawn-out, etc. Motivated by dierentkinds of applications, techniques have also been developed to deal with data of various types:binary, nominal and other kinds of discrete variables, continuous variables, similarities, anddissimilarities. See [10, 17] for more detailed discussions and analyses of these issues.Existing clustering algorithms can be classied into two main categories: hierarchicalmethods and partitioning methods. Hierarchical methods are either agglomerative or divi-sive. Given n objects to be clustered, agglomerative methods begin with n clusters (i.e. allobjects are apart). In each step, two clusters are chosen and merged. This process continuesuntil all objects are clustered into one group. On the other hand, divisive methods beginby putting all objects in one cluster. In each step, a cluster is chosen and split up into two.This process continues until n clusters are produced. While hierarchical methods have beensuccessfully applied to many biological applications (e.g. for producing taxonomies of ani-mals and plants [10]), they are well known to suer from the weakness that they can neverundo what was done previously. Once an agglomerative method merges two objects, theseobjects will always be in one cluster. And once a divisive method separates two objects,these objects will never be re-grouped into the same cluster.In contrast, given the number k of partitions to be found, a partitioning method tries tond the best k partitions 1 of the n objects. It is very often the case that the k clusters foundby a partitioning method are of higher quality (i.e. more similar) than the k clusters producedby a hierarchicalmethod. Because of this property, developing partitioning methods has been1Partitions here are dened in the usual way: each object is assigned to exactly one group.3
one of the main focuses of cluster analysis research. Indeed, many partitioning methods havebeen developed, some based on k-means, some on k-medoid, some on fuzzy analysis, etc.Among them, we have chosen the k-medoid methods as the basis of our algorithm for thefollowing reasons. First, unlike many other partitioning methods, the k-medoid methods arevery robust to the existence of outliers (i.e. data points that are very far away from therest of the data points). Second, clusters found by k-medoid methods do not depend on theorder in which the objects are examined. Furthermore, they are invariant with respect totranslations and orthogonal transformations of data points. Last but not least, experimentshave shown that the k-medoid methods described below can handle very large data setsquite eciently. See [10] for a more detailed comparison of k-medoid methods with otherpartitioning methods. In the remainder of this section, we present the two best-knownk-medoid methods on which our algorithm is based.2.2 PAMPAM (Partitioning Around Medoids) was developed by Kaufman and Rousseeuw [10]. Tond k clusters, PAM's approach is to determine a representative object for each cluster.This representative object, called a medoid, is meant to be the most centrally located objectwithin the cluster. Once the medoids have been selected, each non-selected object is groupedwith the medoid to which it is the most similar. More precisely, if Oj is a non-selectedobject, and Oi is a (selected) medoid, we say that Oj belongs to the cluster represented byOi, if d(Oj ; Oi) = minOed(Oj ; Oe), where the notation minOe denotes the minimum over allmedoids Oe, and the notation d(Oa; Ob) denotes the dissimilarity or distance between objectsOa and Ob. All the dissimilarity values are given as inputs to PAM. Finally, the quality ofa clustering (i.e. the combined quality of the chosen medoids) is measured by the averagedissimilarity between an object and the medoid of its cluster.To nd the k medoids, PAM begins with an arbitrary selection of k objects. Then in eachstep, a swap between a selected object Oi and a non-selected object Oh is made, as long assuch a swap would result in an improvement of the quality of the clustering. In particular,to calculate the eect of such a swap between Oi and Oh, PAM computes costs Cjih for allnon-selected objects Oj. Depending on which of the following cases Oj is in, Cjih is denedby one of the equations below.First Case: suppose Oj currently belongs to the cluster represented by Oi. Furthermore,let Oj be more similar to Oj;2 than Oh, i.e. d(Oj ; Oh)  d(Oj ; Oj;2), where Oj;2 is the secondmost similar medoid to Oj . Thus, if Oi is replaced by Oh as a medoid, Oj would belong tothe cluster represented by Oj;2. Hence, the cost of the swap as far as Oj is concerned is:Cjih = d(Oj ; Oj;2)  d(Oj ; Oi): (1)This equation always gives a non-negative Cjih, indicating that there is a non-negative costincurred in replacing Oi with Oh.Second Case: Oj currently belongs to the cluster represented by Oi. But this time, Oj isless similar to Oj;2 than Oh, i.e. d(Oj ; Oh) < d(Oj ; Oj;2). Then, if Oi is replaced by Oh, Ojwould belong to the cluster represented by Oh. Thus, the cost for Oj is given by:Cjih = d(Oj ; Oh)  d(Oj ; Oi): (2)4
Unlike in Equation (1), Cjih here can be positive or negative, depending on whether Oj ismore similar to Oi or to Oh.Third Case: suppose that Oj currently belongs to a cluster other than the one representedby Oi. Let Oj;2 be the representative object of that cluster. Furthermore, let Oj be moresimilar to Oj;2 than Oh. Then even if Oi is replaced by Oh, Oj would stay in the clusterrepresented by Oj;2. Thus, the cost is: Cjih = 0: (3)Fourth Case: Oj currently belongs to the cluster represented by Oj;2. But Oj is lesssimilar to Oj;2 than Oh. Then replacing Oi with Oh would cause Oj to jump to the clusterof Oh from that of Oj;2. Thus, the cost is:Cjih = d(Oj ; Oh)  d(Oj ; Oj;2); (4)and is always negative. Combining the four cases above, the total cost of replacing Oi withOh is given by: TCih = Xj Cjih (5)We now present Algorithm PAM.Algorithm PAM1. Select k representative objects arbitrarily.2. Compute TCih for all pairs of objects Oi; Oh where Oi is currently selected, and Oh isnot.3. Select the pair Oi; Oh which corresponds to minOi;Oh TCih. If the minimum TCih isnegative, replace Oi with Oh, and go back to Step (2).4. Otherwise, for each non-selected object, nd the most similar representative object.Halt. 2Experimental results show that PAM works satisfactorily for small data sets (e.g. 100 objectsin 5 clusters [10]). But it is not ecient in dealing with medium and large data sets. This isnot too surprising if we perform a complexity analysis on PAM. In Steps (2) and (3), there arealtogether k(n  k) pairs of Oi; Oh. For each pair, computing TCih requires the examinationof (n   k) non-selected objects. Thus, Steps (2) and (3) combined is of O(k(n   k)2). Andthis is the complexity of only one iteration. Thus, it is obvious that PAM becomes too costlyfor large values of n and k. This analysis motivates the development of CLARA.2.3 CLARADesigned by Kaufman and Rousseeuw to handle large data sets, CLARA (Clustering LARgeApplications) relies on sampling [10]. Instead of nding representative objects for the entiredata set, CLARA draws a sample of the data set, applies PAM on the sample, and nds the5
medoids of the sample. The point is that if the sample is drawn in a suciently randomway, the medoids of the sample would approximate the medoids of the entire data set. Tocome up with better approximations, CLARA draws multiple samples and gives the bestclustering as the output. Here, for accuracy, the quality of a clustering is measured based onthe average dissimilarity of all objects in the entire data set, and not only of those objectsin the samples. Experiments reported in [10] indicate that 5 samples of size 40 + 2k givesatisfactory results.Algorithm CLARA1. For i = 1 to 5, repeat the following steps:2. Draw a sample of 40+2k objects randomly from the entire data set 2, and call AlgorithmPAM to nd k medoids of the sample.3. For each object Oj in the entire data set, determine which of the k medoids is the mostsimilar to Oj.4. Calculate the average dissimilarity of the clustering obtained in the previous step. Ifthis value is less than the current minimum, use this value as the current minimum,and retain the k medoids found in Step (2) as the best set of medoids obtained so far.5. Return to Step (1) to start the next iteration. 2Complementary to PAM, CLARA performs satisfactorily for large data sets (e.g. 1000objects in 10 clusters). Recall from Section 2.2 that each iteration of PAM is of O(k(n k)2).But for CLARA, by applying PAM just to the samples, each iteration is of O(k(40 + k)2 +k(n  k)). This explains why CLARA is more ecient than PAM for large values of n.3 A Clustering Algorithm based on Randomized SearchIn this section, we will present our clustering algorithm { CLARANS (Clustering LargeApplications based on RANdomized Search). We will rst introduce CLARANS by givinga graph abstraction of it. Then after describing the details of the algorithm, we will presentexperimental results showing that CLARANS outperforms CLARA and PAM in terms ofboth eciency and eectiveness. In the next section, we will show how CLARANS can beused to provide eective spatial data mining.3.1 Motivation of CLARANS: a Graph AbstractionGiven n objects, the process described above of nding k medoids can be viewed abstractly assearching through a certain graph. In this graph, denoted by Gn;k, a node is represented bya set of k objects fOm1 ; : : : ; Omkg, intuitively indicating that Om1 ; : : : ; Omk are the selectedmedoids. The set of nodes in the graph is the set f fOm1 ; : : : ; Omkg j Om1 ; : : : ; Omk areobjects in the data setg.2[10] reports a useful heuristic to draw samples. Apart from the rst sample, subsequent samples includethe best set of medoids found so far. In other words, apart from the rst iteration, subsequent iterationsdraw 40+ k objects to add on to the best k medoids.6
Two nodes are neighbors (i.e. connected by an arc) if their sets dier by only one object.More formally, two nodes S1 = fOm1 ; : : : ; Omkg and S2 = fOw1 ; : : : ; Owkg are neighbors ifand only if the cardinality of the intersection of S1; S2 is k   1, i.e. jS1 \ S2j = k   1. It iseasy to see that each node has k(n  k) neighbors. Since a node represents a collection of kmedoids, each node corresponds to a clustering. Thus, each node can be assigned a cost thatis dened to be the total dissimilarity between every object and the medoid of its cluster.It is not dicult to see that if objects Oi; Oh are the dierences between neighbors S1 andS2 (i.e. Oi; Oh 62 S1 \ S2, but Oi 2 S1 and Oh 2 S2), the cost dierential between the twoneighbors is exactly given by Tih dened in Equation (5).By now, it is obvious that PAM can be viewed as a search for a minimum on the graphGn;k. At each step, all the neighbors of the current node are examined. The current node isthen replaced by the neighbor with the deepest descent in costs. And the search continuesuntil a minimum is obtained. For large values of n and k (like n = 1000 and k = 10), exam-ining all k(n  k) neighbors of a node is time consuming. This accounts for the ineciencyof PAM for large data sets.On the other hand, CLARA tries to examine fewer neighbors and restricts the search onsubgraphs that are much smaller in size than the original graph Gn;k. However, the problemis that the subgraphs examined are dened entirely by the objects in the samples. Let Sa bethe set of objects in a sample. The subgraph GSa;k consists of all the nodes that are subsets(of cardinalities k) of Sa. Even though CLARA thoroughly examines GSa;k via PAM, thetrouble is that the search is fully conned within GSa;k. If M is the minimum node in theoriginal graph Gn;k , and if M is not included in GSa;k, M will never be found in the searchof GSa;k, regardless of how thorough the search is. To atone for this deciency, many, manysamples would need to be collected and processed.Like CLARA, our algorithm CLARANS does not check every neighbor of a node. Butunlike CLARA, it does not restrict its search to a particular subgraph. In fact, it searchesthe original graph Gn;k. One key dierence between CLARANS and PAM is that the formeronly checks a sample of the neighbors of a node. But unlike CLARA, each sample is drawndynamically in the sense that no nodes corresponding to particular objects are eliminatedoutright. In other words, while CLARA draws a sample of nodes at the beginning of asearch, CLARANS draws a sample of neighbors in each step of a search. This has the benetof not conning a search to a localized area. As will be shown in Section 3.3, a search byCLARANS gives higher quality clusterings than CLARA, and CLARANS requires a verysmall number of searches. We now present the details of Algorithm CLARANS.3.2 CLARANSAlgorithm CLARANS1. Input parameters numlocal and maxneighbor. Initialize i to 1, and mincost to a largenumber.2. Set current to an arbitrary node in Gn;k.3. Set j to 1. 7
4. Consider a random neighbor S of current, and based on Equation (5), calculate thecost dierential of the two nodes.5. If S has a lower cost, set current to S, and go to Step (3).6. Otherwise, increment j by 1. If j  maxneighbor, go to Step (4).7. Otherwise, when j > maxneighbor, compare the cost of current with mincost. If theformer is less than mincost, set mincost to the cost of current, and set bestnode tocurrent.8. Increment i by 1. If i > numlocal, output bestnode and halt. Otherwise, go to Step(2). 2Steps (3) to (6) above search for nodes with progressively lower costs. But if the currentnode has already been compared with the maximum number of the neighbors of the node(specied by maxneighbor) and is still of the lowest cost, the current node is declared to bea \local" minimum. Then in Step (7), the cost of this local minimum is compared with thelowest cost obtained so far. The lower of the two costs above is stored inmincost. AlgorithmCLARANS then repeats to search for other local minima, until numlocal of them have beenfound.As shown above, CLARANS has two parameters: the maximum number of neighborsexamined (maxneighbor), and the number of local minima obtained (numlocal). The higherthe value of maxneighbor, the closer is CLARANS to PAM, and the longer is each search of alocal minima. But the quality of such a local minima is higher, and fewer local minima needsto be obtained. Like many applications of randomized search [8, 9], we rely on experimentsto determine the appropriate values of these parameters.3.3 Experimental Results: Tuning CLARANS3.3.1 Details of ExperimentsTo observe the behavior and eciency of CLARANS, we ran CLARANS with generated datasets whose clusters are known. For better generality, we used two kinds of clusters with quiteopposite characteristics. The rst kind of clusters is rectangular, and the objects within eachcluster are randomly generated. More specically, if such a data set of say 3000 objects in20 clusters is needed, we rst generated 20 \bounding boxes" of the same size. To makethe clusters less clear-cut, the north-east corner of the i-th box and the south-west corner of(i+1)-th box touch. Since for our application of spatial data mining, CLARANS is used tocluster spatial coordinates, objects in our experiments here are pairs of x ; y  coordinates.For each bounding box, we then randomly generated 150 pairs of coordinates that fall withinthe box. Similarly, we generated data sets of the same kind but of varying numbers of objectsand clusters. In the gures below, the symbol rn-k (e.g. r3000-20) represents a data set ofthis kind with n points in k clusters.Unlike the rst kind, the second kind of clusters we experimented with does not containrandom points. Rather, points within a cluster are ordered in a triangle. For example,the points with coordinates (0,0), (1,0), (0,1), (2,0), (1,1), and (0,2) form such a triangular8
cluster of size 6. To produce a cluster next to the previous one, we used a translation of theorigin (e.g. the points (10,10), (11,10), (10,11), (12,10), (11,11), and (10,12)). In the guresbelow, the symbol tn-k (e.g. t3000-20) represents a data set organized in this way with npoints in k clusters.All the experiments reported here were carried out in a time-sharing SPARC-LX work-station. Because of the random nature of CLARANS, all the gures concerning CLARANSare average gures obtained by running the same experiment 10 times (with dierent seedsof the random number generator).3.3.2 Determining the Maximum Number of NeighborsIn the rst series of experiments, we applied CLARANS with the parametermaxneighbor =250, 500, 750, 1000, and 10000 on the data sets rn-k and tn-k, where n varies from 100 to3000 and k varies from 5 to 20. To save space, we only summarize the two major ndingsthat lead to further experiments: When the maximum number of neighbors maxneighbor is set to 10000, the qualityof the clustering produced by CLARANS is eectively the same as the quality of theclustering produced by PAM (i.e. maxneighbor = k(n k)). While we will explain thisphenomenon very shortly, we use the results for maxneighbor = 10000 as a yardstickfor evaluating other (smaller) values of maxneighbor. More specically, the runtimevalues of the rst graph and the average distance values (i.e. quality of a clustering)of the second graph in Figure 1 below are normalized by those produced by settingmaxneighbor = 10000. This explains the two horizontal lines at y  value = 1 in bothgraphs. As expected, a lower value of maxneighbor produces a lower quality clustering. Aquestion we ask is then how small can the value of maxneighbor be before the qualityof the clustering becomes unacceptable. From the rst series of experiments, we ndout that these critical values seem to be proportional to the value k(n   k). Thismotivates us to conduct another series of experiments with the following enhancedformula for determining the value of maxneighbor:if k(n k)  minmaxneighbor thenmaxneighbor = k(n k); otherwise,maxneighborequals the the larger value between p% of k(n  k) and minmaxneighbor.The above formula allows CLARANS to examine all the neighbors as long as the total numberof neighbors is below the threshold minmaxneighbor. Beyond the threshold, the percentageof neighbors examined gradually drops from 100% to a minimum of p%. The two graphs inFigure 1 show the relative runtime and quality of CLARANS with minmaxneighbor = 250and p varying from 1% to 2%. While the graphs only show the results of the rectangulardata sets with 2000 and 3000 points in 20 clusters, these graphs are representative, as theappearances of the graphs for small and medium data sets, and for the triangular data setsare very similar.Figure 1(a) shows that the lower the value of p, the smaller the amount of runtimeCLARANS requires. And as expected, Figure 1(b) shows that a lower value of p producesa lower quality clustering (i.e. higher (relative) average distance). But the very amazing9
(a) relative eciency (b) relative qualityFigure 1: Determining the Maximum Number of Neighborsfeature shown in Figure 1(b) is that the quality is still within 5% from that produced bysetting maxneighbor = 10000 (or by PAM). As an example, if a maximum of p = 1:5% ofneighbors are examined, the quality is within 3%, while the runtime is only 40%. What thatmeans is that examining 98.5% more neighbors, while taking much longer, only producesmarginally better results. This is consistent with our earlier statement that CLARANS withmaxneigh = 10000 gives the same quality as PAM, which is eectively the same as settingmaxneighbor = k(n  k) = 20(3000-20) = 59600.The reason why so few neighbors need to be examined to get good quality clusterings canbe best illustrated by the graph abstraction presented in Section 3.1. Recall that each nodehas k(n   k) neighbors, making the graph very highly connected. Consider two neighborsS1; S2 of the current node, and assume that S1 constitutes a path leading to a certainminimum node S. Even if S1 is missed by not being examined, and S2 becomes the currentnode, there are still numerous paths that connect S2 to S. Of course, if all such paths arenot strictly downward (in cost) paths, and may include \hills" along the way, S will neverbe reached from S2. But our experiments seem to indicate that the chance that a hill existson every path is very small.To keep a good balance between runtime and quality, we believe that a p value between1.25% and 1.5% is very reasonable. For all our later experiments with CLARANS, we chosethe value p = 1:25%.3.3.3 Determining the Number of Local MinimaRecall that Algorithm CLARANS has two parameters: maxneighbor and numlocal. Havingdealt with the former, here we focus on determining the value of numlocal. In this series10
Figure 2: Eciency: CLARANS vs PAMof experiments, we ran CLARANS with numlocal = 1; : : : ; 5 on data sets rn-k and tn-k forsmall, medium and large values of n and k. For each run, we recorded the runtime and thequality of the clustering. The following table (which is typical of all data sets) shows therelative runtime and quality for the data set r2000-20. Here all the values are normalized bythose with numlocal = 5. numlocal 1 2 3 4 5relative runtime 0.19 0.38 0.6 0.78 1relative average distance 1.029 1.009 1 1 1As expected, the runtimes are proportional to the number of local minima obtained. Asfor the relative quality, there is an improvement from numlocal = 1 to numlocal = 2.Performing a second search for a local minimum seems to reduce the impact of \unlucky"randomness that may occur in just one search. However, setting numlocal larger than 2 isnot cost-eective, as there is little increase in quality. This is an indication that a typicallocal minimum is of very high quality. We believe that this phenomenon is largely due to, asdiscussed previously, the peculiar nature of the abstract graph representing the operationsof CLARANS. For all our later experiments with CLARANS, we used the version that ndstwo local minima.3.4 Experimental Results: CLARANS vs PAMIn this series of experiments, we compared CLARANS with PAM. As discussed in Sec-tion 3.3.2, for large and medium data sets, it is obvious that CLARANS, while producingclusterings of very comparable quality, is much more ecient than PAM. Thus, our focushere was to compare the two algorithms on small data sets. We applied both algorithms to11
Figure 3: Relative Quality: Same Time for CLARANS and CLARAdata sets with 40, 60, 80 and 100 points in 5 clusters. Figure 2 shows the runtime taken byboth algorithms. Note that for all those data sets, the clusterings produced by both algo-rithms are of the same quality (i.e. same average distance). Thus, the dierence betweenthe two algorithms is determined by their eciency. It is evident from Figure 2 that even forsmall data sets, CLARANS outperforms PAM signicantly. As expected, the performancegap between the two algorithms grows, as the data set increases in size.3.5 Experimental Results: CLARANS vs CLARAIn this series of experiments, we compared CLARANS with CLARA. As discussed in Sec-tion 2.3, CLARA is not designed for small data sets. Thus, we ran this set of experiments ondata sets whose number of objects exceeds 100. And the objects were organized in dierentnumber of clusters, as well as in the two types of clusters described in Section 3.3.1.When we conducted this series of experiments running CLARA and CLARANS as pre-sented earlier, CLARANS is always able to nd clusterings of better quality than those foundby CLARA. However, in some cases, CLARA may take much less time than CLARANS.Thus, we wondered whether CLARA would produce clusterings of the same quality, if it wasgiven the same amount of time. This leads to the next series of experiments in which wegave both CLARANS and CLARA the same amount of time. Figure 3 shows the quality ofthe clusterings produced by CLARA, normalized by the corresponding value produced byCLARANS.Given the same amount of time, CLARANS clearly outperforms CLARA in all cases. Thegap between CLARANS and CLARA increases from 4% when k, the number of clusters, is5 to 20% when k is 20. This widening of the gap as k increases can be best explained by12
looking at the complexity analyses of CLARA and CLARANS. Recall from Section 2.3 thateach iteration of CLARA is of O(k3+nk). On the other hand, recall from Section 3.3.2 thatthe cost of CLARANS is basically linearly proportional to the number of objects 3. Thus,an increase in k imposes a much larger cost on CLARA than on CLARANS.The above complexity comparison also explains why for a xed number of clusters, thehigher the number of objects, the narrower the gap between CLARANS and CLARA is. Forexample, when the number of objects is 1000, the gap is as high as 30%. The gap drops toaround 20% as the number of object increases to 2000. Since each iteration of CLARA isof O(k3 + nk), the rst term k3 dominates the second term. Thus, for a xed k, CLARA isrelatively less sensitive to an increase in n. On the other hand, since the cost of CLARANSis roughly linearly proportional to n, an increase in n imposes a larger cost on CLARANSthan on CLARA. This explains why for a xed k, the gap narrows as the number of objectsincreases. Nonetheless, the bottom-line shown in Figure 3 is that CLARANS beats CLARAin all cases.In sum, we have presented experimental evidence showing that CLARANS is more ef-cient than PAM and CLARA for small and large data sets. Our experimental results formedium data sets (not included here) lead to the same conclusion.4 Spatial Data Mining based on Clustering AlgorithmsIn this section, we will present two spatial data mining algorithms that use clustering meth-ods. In the next section, we will show experimental results on the eectiveness of thesealgorithms.4.1 Spatial Dominant Approach: SD(CLARANS)There are dierent approaches to spatial data mining. The kind of spatial data miningconsidered in this paper assumes that a spatial database consists of both spatial and non-spatial attributes, and that non-spatial attributes are stored in relations [3, 12, 16]. Thegeneral approach here is to use clustering algorithms to deal with the spatial attributes, anduse other learning tools to take care of the non-spatial counterparts.DBLEARN is the tool we have chosen for mining non-spatial attributes [7]. It takesas inputs relational data, generalization hierarchies for attributes, and a learning queryspecifying the focus of the mining task to be carried out. From a learning request, DBLEARNrst extracts a set of relevant tuples via SQL queries. Then based on the generalizationhierarchies of attributes, it iteratively generalizes the tuples. For example, suppose the tuplesrelevant to a certain learning query have attributes hmajor; ethnicgroupi. Further assume3There is a random aspect and a non-random aspect to the execution of CLARANS. The non-randomaspect corresponds to the part that nds the cost dierential between the current node and its neighbor.This part, as dened in Equation (5) is linearly proportional to the number of objects in the data set. On theother hand, the random aspect corresponds to the part that searches for a local minimum. As the values toplot the graphs are average values of 10 runs, which have the eect of reducing the inuence of the randomaspect, the runtimes of CLARANS used in our graphs are largely dominated by the non-random aspect ofCLARANS. 13
that the generalization hierarchy for ethnicgroup has Indian and Chinese generalized toAsians. Then a generalization operation on the attribute ethnicgroup causes all tuples of theform hm; Indiani and hm;Chinesei to be merged to the tuple hm;Asiansi. This merging hasthe eect of reducing the number of remaining (generalized) tuples. As described in [7], eachtuple has a system-dened attribute called count which keeps track of the number of originaltuples (as stored in the relational database) that are represented by the current (generalized)tuple. This attribute enables DBLEARN to output such statistical statements as 8% of allstudents majoring in Sociology are Asians. In general, a generalization hierarchy may havemultiple levels (e.g. Asians further generalized to non-Canadians), and a learning query mayrequire more than one generalization operation before the nal number of generalized tuplesdrops below a certain threshold 4. At the end, statements such as 90% of all Arts studentsare Canadians may be returned as the ndings of the learning query.Having outlined what DBLEARN does, the specic issue we address here is how to extendDBLEARN to deal with spatial attributes. In particular, we will present two ways to combineclustering algorithms with DBLEARN. The algorithm below, called SD(CLARANS), com-bines CLARANS and DBLEARN in a spatial dominant fashion. That is, spatial clusteringis performed rst, followed by non-spatial generalization of every cluster.Algorithm SD(CLARANS)1. Given a learning request, nd the initial set of relevant tuples by the appropriate SQLqueries.2. Apply CLARANS to the spatial attributes and nd the most natural number knat ofclusters.3. For each of the knat clusters obtained above,(a) collect the non-spatial components of the tuples included in the current cluster.(b) Apply DBLEARN to this collection of the non-spatial components. 2Similarly, Algorithms SD(PAM) and SD(CLARA) can be obtained. But as shown in thelast section that CLARANS is more ecient than PAM and CLARA, the experimentalevaluation to be reported in Section 5 only considers SD(CLARANS).4.2 Determining knat for CLARANSStep (2) of Algorithm SD(CLARANS) tries to nd knat clusters, where knat is the mostnatural number of clusters for the given data set. However, recall that CLARANS and allpartitioning algorithms require the number k of clusters to be given as input. Thus, animmediate question to ask is whether SD(CLARANS) knows beforehand what knat is andcan then simply pass the value of knat to CLARANS. The unfortunate answer is no. In fact,determining knat is one of the most dicult problems in cluster analysis, for which no uniquesolution exists. For SD(CLARANS), we adopt the heuristics of computing the silhouette4Apart from generalization operations (also known as hierarchy ascension operations), DBLEARN, inits full form, may sometimes choose to drop an attribute, if generalizing such an attribute would produceuninteresting results (e.g. generalizing names of students).14
coecients, rst developed by Kaufman and Rousseeuw [10]. (For a survey of alternativecriteria, see [14].) For space considerations, we do not include the formulas for computingsilhouettes, and will only concentrate on how we use silhouettes in our algorithms.Intuitively, the silhouette of an object Oj , a dimensionless quantity varying between  1and 1, indicates how much Oj truly belongs to the cluster to which Oj is classied. Thecloser the value is to 1, the higher the degree Oj belongs to its cluster. The silhouettewidth of a cluster is the average silhouette of all objects in the cluster. Based on extensiveexperimentation, [10] proposes the following interpretation of the silhouette width of acluster: silhouette width interpretation0.71 { 1 the cluster is strong0.51 { 0.7 the cluster is reasonable0.26 { 0.5 the cluster is weak and could be articial 0:25 no cluster can be foundFor a given number k  2 of clusters, the silhouette coecient for k is the average silhouettewidths of the k clusters. Notice that the silhouette coecient does not necessarily decreasemonotonically as k increases 5. If the value k is too small, some distinct clusters are in-correctly grouped together, leading to a small silhouette width. On the other hand, if kis too large, some natural clusters may be articially split, again leading to a small silhou-ette width. Thus, the most natural k is the one whose silhouette coecient is the highest.However, our experiments on spatial data mining show that just using the highest silhou-ette coecient may not lead to intuitive results. For example, some clusters may not havereasonable structures, i.e. widths  0:5. Thus, we use the following heuristics to determinethe value knat for SD(CLARANS).Heuristics for Determining knat1. Find the value k with the highest silhouette coecient.2. If all the k clusters have silhouette widths  0:51, knat = k, and halt.3. Otherwise, remove the objects in those clusters whose silhouette widths are below 0.5,provided that the total number of objects removed so far is less than a threshold (e.g.25% of the total number of objects). The objects removed are considered to be outliersor noises. Go back to Step (1) for the new data set without the outliers.4. If in Step (3), the number of outliers to be removed exceeds the threshold, simply setknat = 1, indicating in eect that no clustering is reasonable. 2In Section 5, we will see the usefulness of the heuristics.As we have completed the description of SD(CLARANS), it is a good time to com-pare SD(CLARANS) with an earlier approach reported in [13] whose goal is to enhance5However, this is not the case for the average dissimilarity of an object from its medoid. The larger thevalue of k, the smaller the average dissimilarity is. This explains why average dissimilarity is only suitableas a measurement criterion for xed k, but is otherwise not suitable to be used to compare the quality ofclusterings produced by dierent k values. 15
DBLEARN with spatial learning capabilities. One of the two proposed approaches there isto rst perform spatial generalizations, and then to use DBLEARN to conduct non-spatialgeneralizations. The fundamental dierence between SD(CLARANS) and that algorithmin [13] is that a user of the latter must give a priori as input generalization hierarchies forspatial attributes. The problem is that without prior analysis, it is almost impossible toguarantee that the given hierarchies are suitable for the given data set. (This may in fact beone of the discoveries to be found out by the spatial data mining task!) For example, sup-pose a spatial data mining request is to be performed on all the expensive houses in GreaterVancouver. A default spatial hierarchy to use may be the one that generalizes streets tocommunities and then to cities. However, if some of the expensive houses are spatially lo-cated along something (such as a river, the bottom of a range of mountains, etc.) thatruns through many communities and cities, then the default spatial hierarchy would be veryineective, generating such general statements as that the expensive houses are more or lessscattered in all the cities in Greater Vancouver.Far extending the capability of the algorithm in [13], SD(CLARANS) nds the clustersdirectly from the given data. To a certain extent, the clustering algorithm, CLARANS inthis case, can be viewed as computing the spatial generalization hierarchy dynamically. Theresult of such computation, combined with the above heuristics to nd knat, precisely ndsthe clusters (if indeed exist in the data set) in terms of the x  and y  coordinates of thepoints, and not conned by any hierarchies specied a priori. For the expensive housesexample discussed above, SD(CLARANS) could directly identify clusters along the river orthe bottom of the mountain range, and could lead to such statements as 80% of all mansionshave either a mountain or a river view. In Section 5, we will see how well our spatial datamining algorithms can handle a data set arguably more complex than the example discussedhere.4.3 Non-Spatial Dominant Approach: NSD(CLARANS)To a large extent, spatial dominant algorithms, such as SD(CLARANS), can be viewedas focusing asymmetrically on discovering non-spatial characterizations of spatial clusters.Non-spatial dominant algorithms, on the other hand, focus on discovering spatial clustersexisting in groups of non-spatial data items. For example, these algorithms may nd inter-esting discoveries based on the spatial clustering or distribution of a certain type of houses.More specically, unlike spatial dominant algorithms, non-spatial dominant algorithms rstapply non-spatial generalizations, followed by spatial clustering. The following algorithm,NSD(CLARANS), uses DBLEARN and CLARANS to perform data mining on non-spatialand spatial attributes respectively.Algorithm NSD(CLARANS)1. Given a learning request, nd the initial set of relevant tuples by the appropriate SQLqueries.2. Apply DBLEARN to the non-spatial attributes, until the nal number of generalizedtuples fall below a certain threshold (cf. Section 4.1).3. For each generalized tuple obtained above,16
(a) collect the spatial components of the tuples represented by the current generalizedtuple.(b) Apply CLARANS and the heuristics presented above to nd the most naturalnumber knat of clusters.4. For all the clusters obtained above, check if there are clusters that intersect or overlap.If exist, such clusters can be merged. This in turn causes the corresponding generalizedtuples to be combined. 2Recall from the previous section on clustering algorithms that for a given data set, clustersdo not overlap or intersect. This is why SD(CLARANS) does not include a step analogous toStep (4) above. However, for NSD(CLARANS) (and other non-spatial dominant algorithmssuch as NSD(PAM)), clusters obtained for dierent generalized tuples can overlap or inter-sect. In that case, opportunities arise for further generalization of spatial and non-spatialdata. This is the purpose of Step (4) above. In the following, we present experimental resultsevaluating the eectiveness of NSD(CLARANS), as well as SD(CLARANS).5 Evaluation of SD(CLARANS) and NSD(CLARANS)5.1 A Real Estate Data SetOne way to evaluate the eectiveness of a data mining algorithm is to apply it to a realdata set and see what it nds. But sometimes it may be dicult to judge the quality ofthe ndings, without knowing a priori what the algorithm is supposed to nd. Thus, toevaluate our algorithms, we generated a data set that honors several rules applicable to the2500 expensive housing units in Vancouver. These rules, very close to reality to the best ofour knowledge, are as follows:A. house type, price and size:1. If the house type is mansion, the price falls within the range [1500K,3500K], andthe size within the range [6000,10000] square feet.2. If the house type is single-house, the price and size ranges are [800K,1500K] and[3000,7000].3. If the house type is condo, the price and size ranges are [300K,800K] and [1000,2500].For simplicity, we assumed uniform distributions within all the ranges.B. distribution:1. There are 1200 condos uniformly distributed in the Vancouver downtown area {the rectangular region at the top of Figure 4. From now on, this region will bereferred to as Area B1.2. Along Marine Drive, there are about 320 mansions and about 80 single-houses {the stripe at the bottom left-hand corner of Figure 4. This area will be referredto as Area B2. 17
Figure 4: Spatial Distribution of the 2500 Housing Units3. Around Queen Elizabeth Park, there are 800 single-houses { the polygonal areaat the bottom right-hand corner of Figure 4. This area will be referred to as AreaB3.4. Finally, to complicate the situation, there are 100 single-houses uniformly dis-tributed in the rest of Vancouver.5.2 Eectiveness of SD(CLARANS)Based on the heuristics presented in Section 4.2, Step (2) of SD(CLARANS) appropriatelysets the value of knat to 3. The silhouette coecient for knat = 3 is 0.7, indicating thatall 3 clusters are quite strong. Thus, Steps (3) and (4) of the heuristics are not neededin this case. After computing knat, it takes CLARANS about 25 seconds to identify the 3clusters (in a time-sharing SPARC-LX workstation environment). The rst cluster contains832 units all single-houses, 800 of which are those in Area B3 dened in Section 5.1. For thiscluster, DBLEARN in Step (3) of SD(CLARANS) correctly nds the price and size rangesto be [800K,1500K] and [3000,7000]. It also reveals that the prices and sizes are more or lessuniformly distributed.The second cluster contains 1235 units, 1200 of which are condos, and the remainderssingle-houses. It contains all the units in Area B1 introduced in Section 5.1. For thiscluster, DBLEARN nds the condo prices and sizes uniformly distributed within the ranges[300K,800K] and [1000,2500] respectively. It also discovers that the single-house prices andsizes fall within [800K,1500K] and [3000,7000].The third cluster contains 431 units, 320 of which are mansions, and the remainders18
single-houses. This cluster includes all the units along the stripe Area B2. For this clus-ter, DBLEARN nds the mansion prices and sizes uniformly distributed within the ranges[1500K,3500K] and [6000,10000]. As for the single-houses in the cluster, DBLEARN againnds the right ranges.In sum, SD(CLARANS) is very eective. This is due primarily to the clusters found byCLARANS, even in the presence of outliers (cf. B.4 of Section 5.1). Once the appropriateclusters are found, DBLEARN easily identies the non-spatial patterns. Thus, CLARANSand DBLEARN together enable SD(CLARANS) to successfully discover all the rules de-scribed in Section 5.1 that it is supposed to nd.5.3 Eectiveness of NSD(CLARANS)In Step (2) of NSD(CLARANS), DBLEARN nds 12 generalized tuples, 4 for each typeof housing units. Let us rst consider the 4 generalized tuples for mansions. The 4 tuplesrepresent respectively mansions in the following categories: a) price in [1500K,2600K], size in[6000,8500]; b) price in [1500K,2600K], size in [8500,10000]; c) price in [2600K,3500K], sizein [6000,8500]; and d) price in [2600K,3500K], size in [8500,10000]. The 4 graphs in Figure 5show the spatial distributions of the mansions in the four categories. When CLARANS isapplied to the points shown in each of the 4 graphs, 2 clusters are found in each case. Thesilhouette coecients for knat = 2 vary from 0.62 to 0.65. In each graph, points in the twoclusters are represented by either dots or +. As shown quite obviously in Figure 5, whenStep (4) of NSD(CLARANS) is executed, all 4 clusters represented by dots overlap. Theseclusters are merged into one larger region. Similarly, all 4 clusters represented by + aremerged into another region. Furthermore, these two regions intersect, and are merged intoan even bigger region, which is now identical to the stripe Area B2 in Figure 4. Last butnot least, these merges of clusters and regions cause the 4 generalized tuples to be combinedas well. As a result, NSD(CLARANS) nds out that all mansions are located in the stripearea, and have prices and sizes in the ranges [1500K,3500K] and [6000,10000].The 4 tuples for condos correspond respectively to the following categories: a) pricein [300K,600K], size in [1000,1800]; b) price in [300K,600K], size in [1800,2500]; c) pricein [600K,800K], size in [1000,1800]; and d) price in [600K,800K], size in [1800,2500]. Theprocessing of these tuples is very similar to the processing of those for mansions above.The only dierence is that for all 4 tuples, no cluster is found 6, i.e. knat set to 1 inStep (4) of the heuristics in Section 4.2. Thus, in the nal step of NSD(CLARANS), all 4regions/clusters, which overlap, are merged into an area that coincides precisely with Area B1Figure 4. Consequently, NSD(CLARANS) discovers that all (expensive) condos are locatedin the Vancouver downtown area, and have prices and sizes in the ranges [300K,800K] and[1000,2500].The processing of single-houses is the most complicated. The 4 tuples correspond to thecategories: a) price in [1200K,1500K], size in [3000,5500]; b) price in [1200K,1500K], sizein [5500,7000]; c) price in [800K,1200K], size in [3000,5500]; and d) price in [800K,1200K],size in [5500,7000]. When CLARANS is applied to the houses in the category a) (as shownin Figure 6(a)), the highest silhouette coecient is found when the number of clusters is 4.625% is the threshold used in Step (3) of the heuristics in Section 4.2.19
(a) tuple 1 (b) tuple 2
(c) tuple 3 (d) tuple 4Figure 5: Clusters for the 4 Generalized tuples for Mansions20
(a) before removing outliers (b) after removing outliersFigure 6: Spatial Distributions for Category a) of Single-housesHowever, even though the silhouette coecient is above 0.5, the silhouette widths of twoof the clusters are below 0.5. Thus, Step (3) of the heuristics in Section 4.2 is invoked.As a result, 15 out of the original 253 points are removed. Figure 6(b) shows the spatialdistribution of this new collection of points after the outliers are removed. For this newcollection, two clusters are identied: i) along the stripe Area B2 in Figure 4, and ii) aroundArea B3 in Figure 4.The clustering for category d) of single-houses is very similar to the one described above.Again, outliers need to be removed. At the end, 2 clusters are found, which are identical tothe ones listed i) and ii) above.As for categories b) and c) of single-houses, the result is slightly dierent. In both cases,because single-houses are quite sparsely located along the stripe area (i.e. the cluster listed i)above), so as to obtain acceptable knat values, some of the houses along that area are removedas outliers. Consequently, for both categories, 2 clusters are identied: again along Area B2and around Area B3. The only dierence between here and the situation for categories a)and d) is that the clusters along the stripe area are smaller in sizes than expected, becauseof outliers removal.After applying CLARANS to all four categories/tuples of single-houses, NSD(CLARANS)in Step (4) merges overlapping or intersecting clusters. As a result, NSD(CLARANS) dis-covers 2 clusters of single-houses, identical to the ones listed i) and ii) above. While thetotal number of units in cluster ii) is as expected, the total number of units in cluster i) isless. Again, this is due to the removal of outliers. Furthermore, NSD(CLARANS) correctlyidenties the price and size ranges for single-houses to be [800K,1500K] and [3000,7000].21
5.4 SummaryWith respect to the rules listed in Section 5.1, both SD(CLARANS) and NSD(CLARANS)nd most of what they are supposed to nd. In terms of performance and eectiveness,SD(CLARANS) has the edge. As discussed earlier, this is due to CLARANS' success inidentifying the clusters right away. On the other hand, in NSD(CLARANS), performingnon-spatial generalizations divides the entire set of points into dierent groups/tuples. Thismay have the eect of breaking down the tightness of some clusters. Outliers removal maythen be needed to extract reasonable clusters from each group. This procedure, as we haveseen, may weaken the eventual ndings and takes more time. Finally, merging overlappingand intersecting clusters can also be costly.However, to be fair with NSD(CLARANS), the rules described in Section 5.1 are morefavorable to SD(CLARANS). There is a strong emphasis on nding out non-spatial charac-terizations of spatial clusters, which is the focus of spatial dominant algorithms. In contrast,a non-spatial dominant algorithm focuses more on nding spatial clusters within groupsof data items that have been generalized non-spatially. For example, if the spatial dis-tribution of single-houses is primarily determined by their price and size categories, thenNSD(CLARANS) could be more eective than SD(CLARANS).6 Discussions6.1 Exploring Spatial RelationshipsThus far, we have shown that clustering algorithms, such as CLARANS, are very promisingand eective for spatial data mining. But we believe that there is an extra dimensiona clustering algorithm can provide. As discussed in Section 4.2, a clustering algorithmdoes not require any spatial generalization hierarchy to be given, and directly discovers thegroups/clusters that are the most appropriate to the given data. In other words, clusteringcan provide very tight spatial characterizations of the groups. The tightness and specicityof the characterizations provide opportunities for exploring spatial relationships that mayexist between the clusters and other interesting objects.For example, as shown in Section 5.2, SD(CLARANS) nds 3 clusters of expensive hous-ing units (cf. Figure 4). Those 3 clusters can then be overlaid with Vancouver maps ofvarious kinds (e.g. parks, highways, lakes, etc.) The following ndings can be obtained: About 96% of the houses in the rst cluster (as described in Section 5.2) are within0.6km from Queen Elizabeth Park. About 97% of the housing units in the second cluster are located in the Vancouverdowntown area which is adjacent to Stanley Park 7. About 92% of houses in the third cluster are within 0.4km from the western coast lineof Vancouver.7During the summit meeting between Russia and the US in 1993, Clinton dined in Queen Elizabeth Parkand jogged in Stanley Park! 22
The point here is that while SD(CLARANS) or NSD(CLARANS) do not directly nd theabove features (which is the job of another package that can provide such spatial operationsas map overlays), they do produce structures or clusters that can lead to further discoveries.6.2 Towards Building a More General and Ecient Spatial DataMining FrameworkA natural extension to SD(CLARANS) and NSD(CLARANS) will be the integration of thetwo algorithms by performing neither spatial dominant nor non-spatial dominant general-izations, but interleaved or balanced generalizations between spatial and non-spatial compo-nents. At each step, the data mining algorithm may select either a spatial or a non-spatialcomponent to generalize. For example, if a clustering method can detect some high qualityclusters, clustering may be performed rst. These clusters may trigger generalization onnon-spatial components in the next step if such a generalization may group objects intointeresting groups. It is an interesting research issue to study how to compare the quality ofspatial and non-spatial generalizations.A spatial database may be associated with several thematic maps, each of which mayrepresent one kind of spatial data. For example, in a city geographic database, one thematicmap may represent the layout of streets and highways, another may outline the emergencyservice network, and the third one may describe the distribution of educational and recre-ational services. To many applications, it will be very useful if data mining on multiplethematic maps can be conducted simultaneously. This would involve not only clustering,but also other spatial operations such as spatial region growing, overlays and spatial joins.Thus, it is an interesting research issue to study how to provide an eective framework thatintegrates all these operations together for simultaneous mining of multiple maps.There are many kinds of spatial data types, such as regions, points and lines, in spatialdatabases. Clustering methods, as presented here, are most suitable for points or smallregions scattered in a relatively large background. However, it remains an open questionas to how they can be eectively applied to deal with line-typed spatial data, such as toexamine how highways are located in cities.Furthermore, due to the nature of spatial data, noise or irrelevant information is prevalentin spatial databases. The development of a general framework for removing noises andltering out irrelevant data is important to the eectiveness of spatial data mining. It is alsointeresting to nd out what roles approximation and aggregation can play in the framework.7 ConclusionsIn this paper, we have presented a clustering algorithm called CLARANS which is based onrandomized search. We have also developed two spatial data mining algorithms SD(CLARANS)and NSD(CLARANS). Experimental results and analysis indicate that both algorithms areeective, and can lead to discoveries that are dicult to obtain with existing spatial datamining algorithms. Finally, we have presented experimental results showing that CLARANSitself is more ecient than existing clustering methods. Hence, CLARANS has establisheditself as a very promising tool for ecient and eective spatial data mining.23
References[1] R. Agrawal, S. Ghosh, T. Imielinski, B. Iyer, and A. Swami. (1992) An Interval Clas-sier for Database Mining Applications, Proc. 18th VLDB, pp 560{573.[2] R. Agrawal, T. Imielinski, and A. Swami. (1993) Mining Association Rules betweenSets of Items in Large Databases, Proc. 1993 SIGMOD, pp 207{216.[3] W. G. Aref and H. Samet. (1991) Optimization Strategies for Spatial Query Processing,Proc. 17th VLDB, pp. 81-90.[4] A. Borgida and R. J. Brachman. (1993) Loading Data into Description Reasoners,Proc. 1993 SIGMOD, pp 217{226.[5] T. Brinkho and H.-P. Kriegel and B. Seeger. (1993) Ecient Processing of SpatialJoins Using R-trees, Proc. 1993 SIGMOD, pp 237-246.[6] O. Gunther. (1993) Ecient Computation of Spatial Joins, Proc. 9th Data Engineering,pp 50-60.[7] J. Han, Y. Cai and N. Cercone. (1992) Knowledge Discovery in Databases: anAttribute-Oriented Approach, Proc. 18th VLDB, pp. 547{559.[8] Y. Ioannidis and Y. Kang. (1990) Randomized Algorithms for Optimizing Large JoinQueries, Proc. 1990 SIGMOD, pp. 312{321.[9] Y. Ioannidis and E. Wong. (1987) Query Optimization by Simulated Annealing, Proc.1987 SIGMOD, pp. 9{22.[10] L. Kaufman and P.J. Rousseeuw. (1990) Finding Groups in Data: an Introduction toCluster Analysis, John Wiley & Sons.[11] D. Keim and H. Kriegel and T. Seidl. (1994) Supporting Data Mining of LargeDatabases by Visual Feedback Queries, to appear in Proc. 10th Data Engineering,Houston, TX.[12] R. Laurini and D. Thompson. (1992) Fundamentals of Spatial Information Systems,Academic Press.[13] W. Lu, J. Han and B. Ooi. (1993) Discovery of General Knowledge in Large SpatialDatabases, Proc. Far East Workshop on Geographic Information Systems, Singapore,pp. 275-289.[14] G. Milligan and M. Cooper. (1985) An Examination of Procedures for Determining theNumber of Clusters in a Data Set, Psychometrika, 50, pp. 159{179.[15] G. Piatetsky-Shapiro and W. J. Frawley. (1991) Knowledge Discovery in Databases,AAAI/MIT Press.[16] H. Samet. (1990) The Design and Analysis of Spatial Data Structures, Addison-Wesley.24
[17] H. Spath. (1985) Cluster Dissection and Analysis: Theory, FORTRAN programs, Ex-amples, Ellis Horwood Ltd.
25
