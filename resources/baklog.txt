===== Loaded model: resources/dataset_model.dat ========== Loaded model: resources/dataset_model.dat ========== Loaded text data: /home/judajocu/IdeaProjects/textmining_RPC_Proyect/resources/papers/0436c31e9a90203b20a7b9be3aebc4a2d471_cleaned.txt ===== ABSTRACT  Association rule mining is the most important technique in the field of data mining. It aims at extracting interesting correlation, frequent pattern, association or casual structure among set of item in the transaction database or other data repositories. Association rule mining is used in various areas for example Banking, department stores etc. The paper surveys the most recent existing association rule mining techniques using apriori algorithm.  Keywords ===== Loaded model: resources/dataset_model.dat ========== Instance created with reference dataset =====@relation 'Test relation'

@attribute class {association,classification,clustering,prediction}
@attribute text string

@data
?,' ABSTRACT  Association rule mining is the most important technique in the field of data mining. It aims at extracting interesting correlation, frequent pattern, association or casual structure among set of item in the transaction database or other data repositories. Association rule mining is used in various areas for example Banking, department stores etc. The paper surveys the most recent existing association rule mining techniques using apriori algorithm.  Keywords '===== Classified instance =====Class predicted: association===== Loaded text data: /home/judajocu/IdeaProjects/textmining_RPC_Proyect/resources/papers/drummondc_cleaned.txt ========== Loaded model: resources/dataset_model.dat ========== Instance created with reference dataset =====@relation 'Test relation'

@attribute class {association,classification,clustering,prediction}
@attribute text string

@data
?,''===== Classified instance =====Class predicted: association===== Loaded text data: /home/judajocu/IdeaProjects/textmining_RPC_Proyect/resources/papers/0436c31e9a90203b20a7b9be3aebc4a2d471_cleaned.txt ===== ABSTRACT  Association rule mining is the most important technique in the field of data mining. It aims at extracting interesting correlation, frequent pattern, association or casual structure among set of item in the transaction database or other data repositories. Association rule mining is used in various areas for example Banking, department stores etc. The paper surveys the most recent existing association rule mining techniques using apriori algorithm.  Keywords ===== Loaded model: resources/dataset_model.dat ========== Instance created with reference dataset =====@relation 'Test relation'

@attribute class {association,classification,clustering,prediction}
@attribute text string

@data
?,' ABSTRACT  Association rule mining is the most important technique in the field of data mining. It aims at extracting interesting correlation, frequent pattern, association or casual structure among set of item in the transaction database or other data repositories. Association rule mining is used in various areas for example Banking, department stores etc. The paper surveys the most recent existing association rule mining techniques using apriori algorithm.  Keywords '===== Classified instance =====Class predicted: association===== Loaded text data: /home/judajocu/IdeaProjects/textmining_RPC_Proyect/resources/papers/drummondc_cleaned.txt ========== Loaded model: resources/dataset_model.dat ========== Instance created with reference dataset =====@relation 'Test relation'

@attribute class {association,classification,clustering,prediction}
@attribute text string

@data
?,''===== Classified instance =====Class predicted: association===== Loaded text data: /home/judajocu/IdeaProjects/textmining_RPC_Proyect/resources/papers/emcat-mlj99_cleaned.txt ===== Abstract. This paper shows that the accuracy of learned text classifiers can be improved byaugmenting a small number of labeled training documents with a large pool of unlabeled docu-ments. This is important because in many text classification problems obtaining training labelsis expensive, while large quantities of unlabeled documents are readily available.We introduce an algorithm for learning from labeled and unlabeled documents based on thecombination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm firsttrains a classifier using the available labeled documents, and probabilistically labels the unlabeleddocuments. It then trains a new classifier using the labels for all the documents, and iteratesto convergence. This basic EM procedure works well when the data conform to the generativeassumptions of the model. However these assumptions are often violated in practice, and poorperformance can result. We present two extensions to the algorithm that improve classificationaccuracy under these conditions: (1) a weighting factor to modulate the contribution of theunlabeled data, and (2) the use of multiple mixture components per class. Experimental results,obtained using text from three different real-world tasks, show that the use of unlabeled datareduces classification error by up to 30%.Keywords: text classification, Expectation-Maximization, integrating supervised and unsuper-vised learning, combining labeled and unlabeled data, Bayesian learning===== Loaded model: resources/dataset_model.dat ========== Instance created with reference dataset =====@relation 'Test relation'

@attribute class {association,classification,clustering,prediction}
@attribute text string

@data
?,' Abstract. This paper shows that the accuracy of learned text classifiers can be improved byaugmenting a small number of labeled training documents with a large pool of unlabeled docu-ments. This is important because in many text classification problems obtaining training labelsis expensive, while large quantities of unlabeled documents are readily available.We introduce an algorithm for learning from labeled and unlabeled documents based on thecombination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm firsttrains a classifier using the available labeled documents, and probabilistically labels the unlabeleddocuments. It then trains a new classifier using the labels for all the documents, and iteratesto convergence. This basic EM procedure works well when the data conform to the generativeassumptions of the model. However these assumptions are often violated in practice, and poorperformance can result. We present two extensions to the algorithm that improve classificationaccuracy under these conditions: (1) a weighting factor to modulate the contribution of theunlabeled data, and (2) the use of multiple mixture components per class. Experimental results,obtained using text from three different real-world tasks, show that the use of unlabeled datareduces classification error by up to 30\%.Keywords: text classification, Expectation-Maximization, integrating supervised and unsuper-vised learning, combining labeled and unlabeled data, Bayesian learning'===== Classified instance =====Class predicted: classification===== Loaded text data: /home/judajocu/IdeaProjects/textmining_RPC_Proyect/resources/papers/get-another-label-provost-kdd08_cleaned.txt ===== ABSTRACTThis paper addresses the repeated acquisition of labels for dataitems when the labeling is imperfect. We examine the improve-ment (or lack thereof) in data quality via repeated labeling,and focus especially on the improvement of training labelsfor supervised induction. With the outsourcing of small tasksbecoming easier, for example via Rent-A-Coder or Amazon’sMechanical Turk, it often is possible to obtain less-than-expertlabeling at low cost. With low-cost labeling, preparing theunlabeled part of the data can become considerably moreexpensive than labeling. We present repeated-labeling strate-gies of increasing complexity, and show several main results.(i) Repeated-labeling can improve label quality and modelquality, but not always. (ii) When labels are noisy, repeatedlabeling can be preferable to single labeling even in the tradi-tional setting where labels are not particularly cheap. (iii) Assoon as the cost of processing the unlabeled data is not free,even the simple strategy of labeling everything multiple timescan give considerable advantage. (iv) Repeatedly labeling acarefully chosen set of points is generally preferable, and wepresent a robust technique that combines different notionsof uncertainty to select data points for which quality shouldbe improved. The bottom line: the results show clearly thatwhen labeling is not perfect, selective acquisition of multiplelabels is a strategy that data miners should have in theirrepertoire; for certain label-quality/cost regimes, the benefitis substantial.Categories and Subject DescriptorsH.2.8 [Database Applications]: Data mining; I.5.2 [DesignMethodology]: Classifier design and evaluationGeneral TermsAlgorithms, Design, Experimentation, Management, Measure-ment, PerformanceKeywordsdata selection, data preprocessingPermission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies arenot made or distributed for profit or commercial advantage and that copiesbear this notice and the full citation on the first page. To copy otherwise, torepublish, to post on servers or to redistribute to lists, requires prior specificpermission and/or a fee.KDD’08, August 24–27, 2008, Las Vegas, Nevada, USA.Copyright 2008 ACM 978-1-60558-193-4/08/08 ...$5.00.===== Loaded model: resources/dataset_model.dat ========== Instance created with reference dataset =====@relation 'Test relation'

@attribute class {association,classification,clustering,prediction}
@attribute text string

@data
?,' ABSTRACTThis paper addresses the repeated acquisition of labels for dataitems when the labeling is imperfect. We examine the improve-ment (or lack thereof) in data quality via repeated labeling,and focus especially on the improvement of training labelsfor supervised induction. With the outsourcing of small tasksbecoming easier, for example via Rent-A-Coder or Amazon’sMechanical Turk, it often is possible to obtain less-than-expertlabeling at low cost. With low-cost labeling, preparing theunlabeled part of the data can become considerably moreexpensive than labeling. We present repeated-labeling strate-gies of increasing complexity, and show several main results.(i) Repeated-labeling can improve label quality and modelquality, but not always. (ii) When labels are noisy, repeatedlabeling can be preferable to single labeling even in the tradi-tional setting where labels are not particularly cheap. (iii) Assoon as the cost of processing the unlabeled data is not free,even the simple strategy of labeling everything multiple timescan give considerable advantage. (iv) Repeatedly labeling acarefully chosen set of points is generally preferable, and wepresent a robust technique that combines different notionsof uncertainty to select data points for which quality shouldbe improved. The bottom line: the results show clearly thatwhen labeling is not perfect, selective acquisition of multiplelabels is a strategy that data miners should have in theirrepertoire; for certain label-quality/cost regimes, the benefitis substantial.Categories and Subject DescriptorsH.2.8 [Database Applications]: Data mining; I.5.2 [DesignMethodology]: Classifier design and evaluationGeneral TermsAlgorithms, Design, Experimentation, Management, Measure-ment, PerformanceKeywordsdata selection, data preprocessingPermission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies arenot made or distributed for profit or commercial advantage and that copiesbear this notice and the full citation on the first page. To copy otherwise, torepublish, to post on servers or to redistribute to lists, requires prior specificpermission and/or a fee.KDD’08, August 24–27, 2008, Las Vegas, Nevada, USA.Copyright 2008 ACM 978-1-60558-193-4/08/08 ...$5.00.'===== Classified instance =====Class predicted: classification===== Loaded model: resources/dataset_model.dat ========== Loaded text data: /home/judajocu/IdeaProjects/textmining_RPC_Proyect/resources/papers/session2-p1_cleaned.txt ===== Abstract. In this paper, we provide the preliminaries of basic concepts about association rule mining and survey the list of existing association rule mining techniques. Of course, a single article cannot be a complete review of all the al-gorithms, yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions that have yet to be explored. ===== Loaded model: resources/dataset_model.dat ========== Instance created with reference dataset =====@relation 'Test relation'

@attribute class {association,classification,clustering,prediction}
@attribute text string

@data
?,' Abstract. In this paper, we provide the preliminaries of basic concepts about association rule mining and survey the list of existing association rule mining techniques. Of course, a single article cannot be a complete review of all the al-gorithms, yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions that have yet to be explored. '===== Classified instance =====Class predicted: association===== Loaded text data: /home/judajocu/IdeaProjects/textmining_RPC_Proyect/resources/papers/meek02_cleaned.txt ===== AbstractWe examine the learning-curve sampling method, an approach for applying machine-learning algorithms to large data sets. The approach is based on the observation thatthe computational cost of learning a model increases as a function of the sample size of thetraining data, whereas the accuracy of a model has diminishing improvements as a functionof sample size. Thus, the learning-curve sampling method monitors the increasing costsand performance as larger and larger amounts of data are used for training, and termi-nates learning when future costs outweigh future benefits. In this paper, we formalize the===== Loaded model: resources/dataset_model.dat ========== Instance created with reference dataset =====@relation 'Test relation'

@attribute class {association,classification,clustering,prediction}
@attribute text string

@data
?,' AbstractWe examine the learning-curve sampling method, an approach for applying machine-learning algorithms to large data sets. The approach is based on the observation thatthe computational cost of learning a model increases as a function of the sample size of thetraining data, whereas the accuracy of a model has diminishing improvements as a functionof sample size. Thus, the learning-curve sampling method monitors the increasing costsand performance as larger and larger amounts of data are used for training, and termi-nates learning when future costs outweigh future benefits. In this paper, we formalize the'===== Classified instance =====Class predicted: clustering===== Loaded text data: /home/judajocu/IdeaProjects/textmining_RPC_Proyect/resources/papers/drummondc_cleaned.txt ========== Loaded model: resources/dataset_model.dat ========== Instance created with reference dataset =====@relation 'Test relation'

@attribute class {association,classification,clustering,prediction}
@attribute text string

@data
?,''===== Classified instance =====Class predicted: association===== Loaded text data: /home/judajocu/IdeaProjects/textmining_RPC_Proyect/resources/papers/drummondc_cleaned.txt ========== Loaded model: resources/dataset_model.dat ========== Instance created with reference dataset =====@relation 'Test relation'

@attribute class {association,classification,clustering,prediction}
@attribute text string

@data
?,''===== Classified instance =====Class predicted: association===== Loaded text data: /home/judajocu/IdeaProjects/textmining_RPC_Proyect/resources/papers/session2-p1_cleaned.txt ===== Abstract. In this paper, we provide the preliminaries of basic concepts about association rule mining and survey the list of existing association rule mining techniques. Of course, a single article cannot be a complete review of all the al-gorithms, yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions that have yet to be explored. ===== Loaded model: resources/dataset_model.dat ========== Instance created with reference dataset =====@relation 'Test relation'

@attribute class {association,classification,clustering,prediction}
@attribute text string

@data
?,' Abstract. In this paper, we provide the preliminaries of basic concepts about association rule mining and survey the list of existing association rule mining techniques. Of course, a single article cannot be a complete review of all the al-gorithms, yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions that have yet to be explored. '===== Classified instance =====Class predicted: association===== Loaded model: resources/dataset_model.dat ========== Loaded model: resources/dataset_model.dat ========== Loaded model: resources/dataset_model.dat ========== Loaded model: resources/dataset_model.dat ========== Loaded model: resources/dataset_model.dat ========== Loaded model: resources/dataset_model.dat ========== Loaded model: resources/dataset_model.dat ========== Loaded model: resources/dataset_model.dat ========== Loaded model: resources/dataset_model.dat =====