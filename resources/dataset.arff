@relation text_dataset

@attribute textclass{association,classification,clustering,prediction}
@attribute text String

@data

clustering,'Partitioning a large set of objects into homogeneous clusters is afundamental operation in data mining. The k-means algorithm isbest suited for implementing this operation because of itsefficiency in clustering large data sets. However, working only onnumeric values limits its use in data mining because data sets indata mining often contain categorical values. In this paper wepresent an algorithm, called k-modes, to extend the k-meansparadigm to categorical domains. We introduce new dissimilaritymeasures to deal with categorical objects, replace means ofclusters with modes, and use a frequency based method to updatemodes in the clustering process to minimise the clustering costfunction. Tested with the well known soybean disease data setthe algorithm has demonstrated a very good classificationperformance. Experiments on a very large health insurance dataset consisting of half a million records and 34 categoricalattributes show that the algorithm is scalable in terms of both thenumber of clusters and the number of records.'
clustering,'Data warehouses provide a great deal of opportunities for performing data mining tasks such as classification and clustering. Typically, updates are collected and applied to the data warehouse periodically in a batch mode, e.g., during the night. Then, all patterns derived from the warehouse by some data mining algorithm have to be updated as well. Due to the very large size of the databases, it is highly desirable to perform these updates incrementally. In this paper, we present the first incremental clustering algorithm. Our algorithm is based on the clustering algorithmDBSCAN which is applicable to any database containing data from a metric space, e.g., to a spatial database or to a WWW-log database. Due to the density-based nature of DBSCAN, the insertion or deletion of an object affects the current clustering only in the neighborhood of this object. Thus, efficient algorithms can be given for incremental insertions and deletions to an existing clustering. Based on the formal definition of clusters, it can be proven that the incremental algorithm yields the same result as DBSCAN. A performance evaluation of IncrementalDBSCAN on a spatial database as well as on a WWW-log database is presented, demonstrating the efficiency of theproposed algorithm. IncrementalDBSCAN yields significant speed-up factors over DBSCAN even for large numbers of daily updates in a data warehouse.'
association,'Association rule mining is the most important technique in the field of data mining. It aims at extracting interesting correlation, frequent pattern, association or casual structure among set of item in the transaction database or other data repositories. Association rule mining is used in various areas for example Banking, department stores etc. The paper surveys the most recent existing association rule mining techniques using apriori algorithm. Keywords Association rule mining, Ant colony optimization, E-mail detection, Database Reverse Engineering.'
prediction,'This paper presents the  method of mining the data and which contains the information about  the large information about the PR (Panchayat Raj Department) of Orissa .we have focused some of the   techniques ,approaches and  different  methodology’s  of the demand forecasting .Every organizations are operated  in different places of the country. Each place of operation may generate a huge amount of data. In an organization, worker prediction is the difficult task of the manager. It is the complex process not only because its nature of feature prediction but also various approaches methodologies always makes user confused. This paper aims to deal with the problem selection process.In this paper we have used some of the approaches from iteratures are been introduced and analyzed to find its suitable organization and situation, Based on this we have designed with automatic selection function to help users make a prejudgment The information about each approach will be showed to users with examples to help understanding. This system also provides calculation function to help users work out a predication result. Generally the new developed system has a more comprehensive functions compared with existing ones .It aims to improve the accuracy of demand forecasting by implementing the forecasting algorithm. While it is still a decision support system with no ability of make the final judgment. .The data warehouse is used in the significant business value by improving the effectiveness of managerial decision-making. In an uncertain and highly competitive business environment, the value of strategic information systems such as these are easily recognized however in today’s business environment, efficiency or speed is not the only key for competitiveness. This type of huge amount of data’s are available in the form of tera- to peta-bytes which has drastically changed in the areas of science and engineering. To analyze, manage and make a decision of such type of huge amount of data we need techniques called the data mining which will transforming in many fields. We have implemented the algorithms in JAVA technology. This paper provides the prediction algorithm (Linear Regression, result which will helpful in the further research.  Keywords  Data Mining, Classification Based on the Data Mining, Data Mining Forecasting Technique'
classification,'Data mining techniques have numerous applications in malware detection. Classification method is one of the most popular data mining techniques. In this paperwe present a datamining classification approach to detectmalware behavior. Weproposed different classification methods in order to detect malware based on the feature and behavior of each malware. A dynamic analysis method has been presented for identifying themalware features. A suggested programhas been presented for converting amalware behavior executive history XML file to a suitable WEKA tool input. To illustrate the performance efficiency as well as training data and test, we apply the proposed approaches to a real case study data set using WEKA tool. The evaluation results demonstrated the availability of the proposed data mining approach. Also our proposed data mining approach is more efficient for detecting malware and behavioral classification of malware can be useful to detect malware in a behavioral antivirus.'
classification,'This paper takes a new look at two samplingschemes commonly used to adapt machinelearning algorithms to imbalanced classesand misclassification costs. It uses a perfor-mance analysis technique called cost curvesto explore the interaction of over and under-sampling with the decision tree learner C4.5.C4.5 was chosen as, when combined with oneof the sampling schemes, it is quickly becom-ing the community standard when evaluat-ing new cost sensitive learning algorithms.This paper shows that using C4.5 with under-sampling establishes a reasonable standardfor algorithmic comparison. But it is rec-ommended that the cheapest class classifierbe part of that standard as it can be bet-ter than under-sampling for relatively mod-est costs. Over-sampling, however, shows lit-tle sensitivity, there is often little difference in performance when misclassification costsare changed.'
classification,'This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available. We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%. Keywords: text classification, Expectation-Maximization, integrating supervised and unsupervised learning, combining labeled and unlabeled data, Bayesian learning'
classification,'This paper addresses the repeated acquisition of labels for dataitems when the labeling is imperfect. We examine the improve-ment (or lack thereof) in data quality via repeated labeling,and focus especially on the improvement of training labelsfor supervised induction. With the outsourcing of small tasksbecoming easier, for example via Rent-A-Coder or Amazon’sMechanical Turk, it often is possible to obtain less-than-expertlabeling at low cost. With low-cost labeling, preparing theunlabeled part of the data can become considerably moreexpensive than labeling. We present repeated-labeling strate-gies of increasing complexity, and show several main results.(i) Repeated-labeling can improve label quality and modelquality, but not always. (ii) When labels are noisy, repeatedlabeling can be preferable to single labeling even in the tradi-tional setting where labels are not particularly cheap. (iii) Assoon as the cost of processing the unlabeled data is not free,even the simple strategy of labeling everything multiple timescan give considerable advantage. (iv) Repeatedly labeling acarefully chosen set of points is generally preferable, and wepresent a robust technique that combines different notionsof uncertainty to select data points for which quality shouldbe improved. The bottom line: the results show clearly thatwhen labeling is not perfect, selective acquisition of multiplelabels is a strategy that data miners should have in theirrepertoire; for certain label-quality/cost regimes, the benefitis substantial.'
classification,'An approach to the construction of classifiers from imbalanced datasets is described.A dataset is imbalanced if the classification categories are not approximately equally rep-resented. Often real-world data sets are predominately composed of “normal” exampleswith only a small percentage of “abnormal” or “interesting” examples. It is also the casethat the cost of misclassifying an abnormal (interesting) example as a normal example isoften much higher than the cost of the reverse error. Under-sampling of the majority (nor-mal) class has been proposed as a good means of increasing the sensitivity of a classifier tothe minority class. This paper shows that a combination of our method of over-samplingthe minority (abnormal) class and under-sampling the majority (normal) class can achievebetter classifier performance (in ROC space) than only under-sampling the majority class.This paper also shows that a combination of our method of over-sampling the minority classand under-sampling the majority class can achieve better classifier performance (in ROCspace) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our methodof over-sampling the minority class involves creating synthetic minority class examples.Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The methodis evaluated using the area under the Receiver Operating Characteristic curve (AUC) andthe ROC convex hull strategy.'
association,'Based on in-depth study of the existing data mining and association rule mining algorithms, a new mining algorithm of weighted association rules is proposed. By introducing a support factor of the weighted frequent item sets, a reasonable minimum support is set. The algorithm does not need to repeatedly scan the database in the discovery of frequent item sets, so it greatly reduces the time of input and output, and improves the efficiency of data mining.  Keywords-Data mining, Association rules, Apriori algorithm, Frequent item set'
clustering,'We examine the learning-curve sampling method, an approach for applying machine learning algorithms to large data sets. The approach is based on the observation that the computational cost of learning a model increases as a function of the sample size of the training data, whereas the accuracy of a model has diminishing improvements as a function of sample size. Thus, the learning-curve sampling method monitors the increasing costs and performance as larger and larger amounts of data are used for training, and terminates learning when future costs outweigh future benefits. In this paper, we formalize the learning-curve sampling method and its associated cost-benefit tradeoff in terms of decision theory. In addition, we describe the application of the learning-curve sampling method to the task of model-based clustering via the expectation-maximization (EM) algorithm. In experiments on three real data sets, we show that the learning-curve sampling method produces models that are nearly as accurate as those trained on complete data sets, but with dramatically reduced learning times. Finally, we describe an extension of the basic learning-curve approach for model-based clustering that results in an additional speedup. This extension is based on the observation that the shape of the learning curve for a given model and data set is roughly independent of the number of EM iterations used during training. Thus, we run EM for only a few iterations to decide how many cases to use for training, and then run EM to full convergence once the number of cases is selected. Keywords: Learning-curve sampling method, clustering, scalability, decision theory, sampling'
prediction,'To be successful recommender systems must gainthe trust of users. To do this they must demonstratetheir ability to make reliable predictions. We argue that collaborative filtering recommendation algorithms can benefit from explicit models of trustto inform their predictions. We present one suchmodel of trust along with a cost-benefit analysisthat focuses on the classical trade off that exists between recommendation coverage and prediction accuracy.'
prediction,'In many cost sensitive environments class probability estimates are used by decision makers to evaluate the expected utility from a set of alternatives. Supervised learning can be used to build class probability estimates; however, it often is very costly to obtain training data with class labels. Active learning acquires data incrementally, at each phase identifying especially useful additional data for labeling, and can be used to economize on examples needed for learning. We outline the critical features of an active learner and present a sampling-based active learning method for estimating class probabilities and class-based rankings. BOOT-STRAP-LV identifies particularly informative new data for learning based on the variance in probability estimates, and uses weighted sampling to account for a potential example’s informative value for the rest of the input space. We show empirically that the method reduces the number of data items that must be obtained and labeled, across a wide variety of domains. We investigate the contribution of the components of the algorithm and show that each provides valuable information to help identify informative examples. We also compare BOOTSTRAP-LV with UNCERTAINTY SAMPLING, an existing active learning method designed to maximize classification accuracy. The results show that BOOTSTRAP-LV uses fewer examples to exhibit a certain estimation accuracy and provide insights to the behavior of the algorithms. Finally, we experiment with another new active sampling algorithm drawing from both UNCERTAINTY SAMPLING and BOOTSTRAP-LV and show that it is significantly more competitive with BOOTSTRAP-LV compared to UNCERTAINTY SAMPLING. The analysis suggests more general implications for improving existing active sampling algorithms for classification.  Keywords:   active learning, cost-sensitive learning, class probability estimation, rank-ing, supervised learning, decision trees, uncertainty sampling'
association,'In this paper, we provide the preliminaries of basic concepts about association rule mining and survey the list of existing association rule mining techniques. Of course, a single article cannot be a complete review of all the al-gorithms, yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions that have yet to be explored.'
association,'Association rule mining is the most important technique in the field of data mining. Association rule mining finding frequent patterns, associations, correlations, or causal structures among sets of items or objects in transaction databases, relational databases, and other information repositories. In this paper we present a survey of recent research work carried by different researchers. Of course, a single article cannot be a complete review of all the research work, yet we hope that it will provide a guideline for the researcher in interesting research directions that have yet to be explored.   Keywords— Association Rules, Confidence, frequent items,  Item set, Minimum Support..'
association,'Frequent pattern mining is one of the active research themes in data mining. It plays an important role in all data mining tasks such as clustering, classification, prediction, and association analysis. Identifying all frequent patterns is the most time consuming process due to a massive number of patterns generated. In this paper, we present thus study of different data structures used and the algorithms for finding the frequent items in the fastest way. We also present an approach for mining of association rule. Keywords Data mining; Frequentitemsets; Distributed computation, Association rules'
clustering,'Spatial data mining is the discovery of inter-esting relationships and characteristics that may exist implicitly in spatial databases. In this paper, we explore whether clustering methods have a role to play in spatial data mining. To this end, we develop a new clustering method called CLAHANS which is based on randomized search. We also de-velop two spatial data mining algorithms that use CLAHANS. Our analysis and experiments show that with the assistance of CLAHANS, these two algorithms are very effective and can lead to discoveries that are difficult to find with current spatial data mining algo-rithms. Furthermore, experiments conducted to compare the performance of CLAHANS with that of existing clustering methods show that CLAHANS is the most efficient. '
prediction,'Heart disease is a major life threatening disease that cause to death and it has a serious long term disability. There is wealth of data available within the health care system. However, there is lack of effective tools to discover hidden relationships and trends in data .Advanced data mining techniques can help remedial situations. This paper describes about a prototype using data mining techniques mainly Naïve Bayes and WAC (Weighted Associated Classifier). The dataset is composed of important  factors such as age ,sex, diabetic, height, weight,  blood pressure, cholesterol, fasting blood sugar, hypertension, disease. The system indicates whether patient had a risk of heart disease or not. Key Words:  Data mining, Naïve Bayes, WAC, Prediction'
clustering,'Spatial data mining, i.e., discovery of interesting characteristics and patterns that may implicitly exist in spatial databases, is a challenging task due to the huge amounts of spatial data and to the new conceptual nature of the problems which must account for spatial distance. Clustering and region oriented queries are common problems in this domain. Several approaches have been presented in recent years, all of which require at least one scan of all individual objects (points). Consequently, the computational complexity is at least linearly proportional to the number of objects to answer each query. In this paper, we propose a hierarchical statistical information grid based approach for spatial data mining to reduce the cost further. The idea is to capture statistical information associated with spatial cells in such a manner that whole classes of queries and clustering problems can be answered without recourse to the individual objects. In theory, and confirmed by empirical studies, this approach outperforms the best previous method by at least an order of magnitude, specially when the data set is very large.'
prediction,'This paper discusses learning from partially labeled data in the framework of probabilistic supervised clasification. Mininmun commitment logistic regression is a conservative solution to the problem of imprecise labels, which should be appropriate if the faithful estimation of posterior probabilities is an issue. Semi-supervised learning is among the problems considered, and a series of experiments shows that our second proprosal, self-consitent logistic regression is a serious contender to more classical solutions involving generative models. Keywords: partial labels, logistic regression semi-supervised learning'